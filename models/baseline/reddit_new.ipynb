{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Dropout\n",
    "from keras.layers import LSTM, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(df, labels_column_name):\n",
    "    n = df.shape[0]\n",
    "    print(\"{} labels frequency:\".format(labels_column_name))\n",
    "    print(\"Value\\tCount\\tPercent\")\n",
    "    indeces = df[labels_column_name].value_counts().index.tolist()\n",
    "    counts = df[labels_column_name].value_counts().tolist()\n",
    "    for val, count in zip(indeces, counts):\n",
    "        print(\"{}\\t{}\\t{}%\".format(val, count, (count / float(n)) * 100))\n",
    "    \n",
    "def get_max_words(text_arr):\n",
    "    max_words = 0\n",
    "    for line in text_arr:\n",
    "        num_words = len(line.split())\n",
    "        if num_words > max_words:\n",
    "            max_words = num_words\n",
    "    return max_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the csv data\n",
    "reddit_train_df = pd.read_csv(\"../../data/reddit/labeled/score10_all_sub_labeled_train.csv\", index_col=0)\n",
    "reddit_test_df = pd.read_csv(\"../../data/reddit/labeled/score10_all_sub_labeled_dev.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words per post: 78\n",
      "Train Set Distributions:\n",
      "\n",
      "bitcoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t54066\t57.88341095230448%\n",
      "0\t39339\t42.11658904769552%\n",
      "ethereum_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t50802\t54.388951340934646%\n",
      "0\t42603\t45.61104865906536%\n",
      "litecoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t48582\t52.012204914083824%\n",
      "0\t44823\t47.987795085916176%\n",
      "\n",
      "Test Set Distributions:\n",
      "\n",
      "bitcoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t7242\t53.4307215582116%\n",
      "0\t6312\t46.569278441788406%\n",
      "ethereum_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t7355\t54.26442378633614%\n",
      "0\t6199\t45.73557621366386%\n",
      "litecoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "0\t7138\t52.66342039250406%\n",
      "1\t6416\t47.33657960749594%\n",
      "\n",
      "Getting x_train, y_train, x_test, and y_test...\n",
      "93405 train sequences\n",
      "13554 test sequences\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Determine max post length\n",
    "max_words_train = get_max_words(reddit_train_df.title.values)\n",
    "max_words_test = get_max_words(reddit_test_df.title.values)\n",
    "max_words = max(max_words_train, max_words_test)\n",
    "print(\"Max number of words per post: {}\".format(max_words))\n",
    "\n",
    "# Label and title columns in datasets\n",
    "BTC_LABEL_COL, ETH_LABEL_COL, LTC_LABEL_COL = 'bitcoin_one', 'ethereum_one', 'litecoin_one'\n",
    "TEXT_COL = 'title'\n",
    "\n",
    "# Print info about each label\n",
    "print(\"{} Set Distributions:\\n\".format('Train'))\n",
    "print_distribution(reddit_train_df, BTC_LABEL_COL)\n",
    "print_distribution(reddit_train_df, ETH_LABEL_COL)\n",
    "print_distribution(reddit_train_df, LTC_LABEL_COL)\n",
    "print(\"\\n{} Set Distributions:\\n\".format('Test'))\n",
    "print_distribution(reddit_test_df, BTC_LABEL_COL)\n",
    "print_distribution(reddit_test_df, ETH_LABEL_COL)\n",
    "print_distribution(reddit_test_df, LTC_LABEL_COL)\n",
    "\n",
    "# Split into x_train and y_train\n",
    "print('\\nGetting x_train, y_train, x_test, and y_test...')\n",
    "(x_train, y_train_btc, y_train_eth, y_train_ltc) = reddit_train_df[TEXT_COL].values, \\\n",
    "    reddit_train_df[BTC_LABEL_COL], reddit_train_df[ETH_LABEL_COL], reddit_train_df[LTC_LABEL_COL]\n",
    "(x_test, y_test_btc, y_test_eth, y_test_ltc) = reddit_test_df[TEXT_COL].values, \\\n",
    "    reddit_test_df[BTC_LABEL_COL], reddit_test_df[ETH_LABEL_COL], reddit_test_df[LTC_LABEL_COL]\n",
    "\n",
    "# Print info about train and test\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup (part 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (93405, 78)\n",
      "x_test shape: (13554, 78)\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "max_features = 200000 # Num words in our vocabulary \n",
    "maxlen = max_words  # cut texts after this number of words\n",
    "batch_size = 32  # Mini-batch size\n",
    "epochs = 10 \n",
    "\n",
    "# Train tokenizer to create a vocabulary of words\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Vectorize each headline\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Update x_train and x_test to be 'sequences' of data\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup (part 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding layer using word2vec\n",
    "EMBEDDING_FILE = \"../../data/embeddings/GoogleNews-vectors-negative300.bin\"\n",
    "EMBEDDING_DIM = 300\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        \n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=maxlen,\n",
    "        trainable=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup (part 3/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual embeddings\n",
    "sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "embeddings = embedding_layer(sequence_input)\n",
    "\n",
    "# Construct the model (attempt 1 - didn't work)\n",
    "#X = LSTM(128, return_sequences=True)(embeddings)\n",
    "#X = Dropout(0.5)(X)\n",
    "#X = LSTM(128, return_sequences=False)(X)\n",
    "#X = Dropout(0.5)(X)\n",
    "#X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "# Construct the model (attempt 1 - didn't work)\n",
    "X = LSTM(128, return_sequences=False)(embeddings)\n",
    "X = Dense(1, activation='sigmoid')(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 78, 300)           11135400  \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 11,355,177\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 11,135,400\n",
      "_________________________________________________________________\n",
      "Train on 93405 samples, validate on 13554 samples\n",
      "Epoch 1/10\n",
      "93405/93405 [==============================] - 356s 4ms/step - loss: 0.6815 - acc: 0.5784 - val_loss: 0.6940 - val_acc: 0.5343\n",
      "Epoch 2/10\n",
      "93405/93405 [==============================] - 335s 4ms/step - loss: 0.6801 - acc: 0.5785 - val_loss: 0.6933 - val_acc: 0.5342\n",
      "Epoch 3/10\n",
      "93405/93405 [==============================] - 312s 3ms/step - loss: 0.6782 - acc: 0.5795 - val_loss: 0.6961 - val_acc: 0.5348\n",
      "Epoch 4/10\n",
      "93405/93405 [==============================] - 309s 3ms/step - loss: 0.6755 - acc: 0.5821 - val_loss: 0.6973 - val_acc: 0.5327\n",
      "Epoch 5/10\n",
      "93405/93405 [==============================] - 440s 5ms/step - loss: 0.6673 - acc: 0.5935 - val_loss: 0.7055 - val_acc: 0.5275\n",
      "Epoch 6/10\n",
      "93405/93405 [==============================] - 393s 4ms/step - loss: 0.6531 - acc: 0.6093 - val_loss: 0.7194 - val_acc: 0.5212\n",
      "Epoch 7/10\n",
      "93405/93405 [==============================] - 364s 4ms/step - loss: 0.6277 - acc: 0.6396 - val_loss: 0.7416 - val_acc: 0.5205\n",
      "Epoch 8/10\n",
      "93405/93405 [==============================] - 364s 4ms/step - loss: 0.5911 - acc: 0.6745 - val_loss: 0.7800 - val_acc: 0.5173\n",
      "Epoch 9/10\n",
      "93405/93405 [==============================] - 368s 4ms/step - loss: 0.5462 - acc: 0.7107 - val_loss: 0.8547 - val_acc: 0.5077\n",
      "Epoch 10/10\n",
      "93405/93405 [==============================] - 311s 3ms/step - loss: 0.4971 - acc: 0.7435 - val_loss: 0.9317 - val_acc: 0.5169\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-93a2c8d4d67c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m               validation_data=(x_test, y_test))\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m score, acc = model.evaluate(x_test, \n\u001b[0m\u001b[1;32m     22\u001b[0m                             \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the BTC model\n",
    "model_btc = Model(inputs=sequence_input, outputs=X)\n",
    "\n",
    "# Compile the BTC model\n",
    "model_btc.summary()\n",
    "model_btc.compile(loss='binary_crossentropy', \n",
    "                    optimizer='adam', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "#loss = mean square error for regression\n",
    "\n",
    "# Select BTC labels for y\n",
    "y_train = y_train_btc\n",
    "y_test = y_test_btc\n",
    "\n",
    "# Run the BTC model\n",
    "model_btc.fit(x_train, \n",
    "              y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, \n",
    "                            y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue running BTC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 93405 samples, validate on 13554 samples\n",
      "Epoch 1/5\n",
      "93405/93405 [==============================] - 340s 4ms/step - loss: 0.4016 - acc: 0.8048 - val_loss: 1.0923 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "93405/93405 [==============================] - 370s 4ms/step - loss: 0.3602 - acc: 0.8289 - val_loss: 1.2382 - val_acc: 0.5120\n",
      "Epoch 3/5\n",
      "93405/93405 [==============================] - 375s 4ms/step - loss: 0.3240 - acc: 0.8498 - val_loss: 1.3520 - val_acc: 0.5115\n",
      "Epoch 4/5\n",
      "93405/93405 [==============================] - 389s 4ms/step - loss: 0.2922 - acc: 0.8684 - val_loss: 1.4191 - val_acc: 0.5124\n",
      "Epoch 5/5\n",
      "93405/93405 [==============================] - 397s 4ms/step - loss: 0.2661 - acc: 0.8832 - val_loss: 1.5705 - val_acc: 0.5083\n",
      "13554/13554 [==============================] - 14s 1ms/step\n",
      "Test score: 1.5704745515637892\n",
      "Test accuracy: 0.5082632432966189\n"
     ]
    }
   ],
   "source": [
    "# Run the BTC model\n",
    "model_btc.fit(x_train, \n",
    "              y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=5, \n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model_btc.evaluate(x_test, \n",
    "                            y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
