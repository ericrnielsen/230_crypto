{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1) Load data\n",
    "    - Load both train and test datasets\n",
    "    - Print info about each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 320935 reddit posts for TRAIN\n",
      "There are 75157 reddit posts for TEST\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>title</th>\n",
       "      <th>bitcoin_one</th>\n",
       "      <th>bitcoin_two</th>\n",
       "      <th>ethereum_one</th>\n",
       "      <th>ethereum_two</th>\n",
       "      <th>litecoin_one</th>\n",
       "      <th>litecoin_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320936</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>bitcoinmarkets</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>Addon for Crypto Prices on Binance (firefox an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320937</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>cryptocurrency</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Nice read on IOTA on hacked.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320938</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>cryptocurrency</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>@iotatokennews is a fake account, no new excha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320939</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>cryptocurrency</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>MyWish vs Blockcat vs Etherparty Comparison</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320940</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>cryptocurrency</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NUCLEUS VISION ICO Review! IoT-based, Contactl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date       subreddit  score  num_comments  \\\n",
       "320936  01/01/2018  bitcoinmarkets      4            11   \n",
       "320937  01/01/2018  cryptocurrency     13             0   \n",
       "320938  01/01/2018  cryptocurrency      8             5   \n",
       "320939  01/01/2018  cryptocurrency      3             1   \n",
       "320940  01/01/2018  cryptocurrency      5             1   \n",
       "\n",
       "                                                    title  bitcoin_one  \\\n",
       "320936  Addon for Crypto Prices on Binance (firefox an...            1   \n",
       "320937                    Nice read on IOTA on hacked.com            1   \n",
       "320938  @iotatokennews is a fake account, no new excha...            1   \n",
       "320939        MyWish vs Blockcat vs Etherparty Comparison            1   \n",
       "320940  NUCLEUS VISION ICO Review! IoT-based, Contactl...            1   \n",
       "\n",
       "        bitcoin_two  ethereum_one  ethereum_two  litecoin_one  litecoin_two  \n",
       "320936            1             1             1             0             0  \n",
       "320937            1             1             1             0             0  \n",
       "320938            1             1             1             0             0  \n",
       "320939            1             1             1             0             0  \n",
       "320940            1             1             1             0             0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load in the csv data\n",
    "reddit_train_df = pd.read_csv(\"../../data/reddit/labeled/all_sub_labeled_train.csv\", index_col=0)\n",
    "reddit_test_df = pd.read_csv(\"../../data/reddit/labeled/all_sub_labeled_dev.csv\", index_col=0)\n",
    "\n",
    "print(\"There are {} reddit posts for TRAIN\".format(reddit_train_df.shape[0]))\n",
    "reddit_train_df.head()\n",
    "print(\"There are {} reddit posts for TEST\".format(reddit_test_df.shape[0]))\n",
    "reddit_test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2) Supporting functions\n",
    "    - For printing label distributions\n",
    "    - For determining max length of post titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(df, labels_column_name):\n",
    "    n = df.shape[0]\n",
    "    print(\"{} labels frequency:\".format(labels_column_name))\n",
    "    print(\"Value\\tCount\\tPercent\")\n",
    "    indeces = df[labels_column_name].value_counts().index.tolist()\n",
    "    counts = df[labels_column_name].value_counts().tolist()\n",
    "    for val, count in zip(indeces, counts):\n",
    "        print(\"{}\\t{}\\t{}%\".format(val, count, (count / float(n)) * 100))\n",
    "    \n",
    "def get_max_words(text_arr):\n",
    "    max_words = 0\n",
    "    for line in text_arr:\n",
    "        num_words = len(line.split())\n",
    "        if num_words > max_words:\n",
    "            max_words = num_words\n",
    "    return max_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3) Figure out some stuff about our data\n",
    "    - What is the max number of words from all the reddit posts?\n",
    "        - Need to know this for when we vectorize the words, we need to pad the vectors to all be the same length\n",
    "    - What are the distributions of each data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words per post: 78\n"
     ]
    }
   ],
   "source": [
    "max_words_train = get_max_words(reddit_train_df.title.values)\n",
    "max_words_test = get_max_words(reddit_test_df.title.values)\n",
    "max_words = max(max_words_train, max_words_test)\n",
    "\n",
    "print(\"Max number of words per post: {}\".format(max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Distributions:\n",
      "\n",
      "bitcoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t187970\t58.5694922648%\n",
      "0\t132965\t41.4305077352%\n",
      "ethereum_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t172616\t53.7853459423%\n",
      "0\t148319\t46.2146540577%\n",
      "litecoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t170138\t53.0132269774%\n",
      "0\t150797\t46.9867730226%\n",
      "\n",
      "Test Set Distributions:\n",
      "\n",
      "bitcoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t40244\t53.5465758346%\n",
      "0\t34913\t46.4534241654%\n",
      "ethereum_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "1\t42176\t56.1171946725%\n",
      "0\t32981\t43.8828053275%\n",
      "litecoin_one labels frequency:\n",
      "Value\tCount\tPercent\n",
      "0\t38514\t51.2447277033%\n",
      "1\t36643\t48.7552722967%\n",
      "\n",
      "Getting x_train, y_train, x_test, and y_test...\n",
      "(320935, 'train sequences')\n",
      "(75157, 'test sequences')\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Label and title columns in datasets\n",
    "BTC_LABEL_COL, ETH_LABEL_COL, LTC_LABEL_COL = 'bitcoin_one', 'ethereum_one', 'litecoin_one'\n",
    "TEXT_COL = 'title'\n",
    "\n",
    "# Print info about each label\n",
    "print(\"{} Set Distributions:\\n\".format('Train'))\n",
    "print_distribution(reddit_train_df, BTC_LABEL_COL)\n",
    "print_distribution(reddit_train_df, ETH_LABEL_COL)\n",
    "print_distribution(reddit_train_df, LTC_LABEL_COL)\n",
    "print(\"\\n{} Set Distributions:\\n\".format('Test'))\n",
    "print_distribution(reddit_test_df, BTC_LABEL_COL)\n",
    "print_distribution(reddit_test_df, ETH_LABEL_COL)\n",
    "print_distribution(reddit_test_df, LTC_LABEL_COL)\n",
    "\n",
    "# Print info about train and test set sizes\n",
    "print('\\nGetting x_train, y_train, x_test, and y_test...')\n",
    "(x_train, y_train_btc, y_train_eth, y_train_ltc) = reddit_train_df[TEXT_COL].values, \\\n",
    "    reddit_train_df[BTC_LABEL_COL], reddit_train_df[ETH_LABEL_COL], reddit_train_df[LTC_LABEL_COL]\n",
    "(x_test, y_test_btc, y_test_eth, y_test_ltc) = reddit_test_df[TEXT_COL].values, \\\n",
    "    reddit_test_df[BTC_LABEL_COL], reddit_test_df[ETH_LABEL_COL], reddit_test_df[LTC_LABEL_COL]\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Setup the basics for the initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, Input\n",
    "\n",
    "\n",
    "######### Hyperparameters\n",
    "max_features = 20000 # Controls the number of words in our vocabulary \n",
    "maxlen = max_words  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32  # mini-batch size\n",
    "epochs = 6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the data by using Keras Tokenizer \n",
    "    - similar to the sklearn CountVectorizer we used before, but more powerful\n",
    "    - https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (320935, 78))\n",
      "('x_test shape:', (75157, 78))\n"
     ]
    }
   ],
   "source": [
    "# First train our Tokenizer to create a vocabulary of words\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Second vectorize each headline\n",
    "# Might want to train a different tokenizer on the test set?\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# For an RNN, ou need a 'sequence' of data as the input\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Model for BTC predictions (using Keras functional API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 78, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Functional API version of the same model\n",
    "# The functional API is much more useful than the sequential API in terms of adaptability \n",
    "input_layer = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "\n",
    "embedding_layer = Embedding(output_dim=128, input_dim=max_features, input_length=maxlen)(input_layer)\n",
    "\n",
    "lstm_layer = LSTM(128)(embedding_layer)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, \n",
    "              outputs=output)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320935 samples, validate on 75157 samples\n",
      "Epoch 1/6\n",
      "320935/320935 [==============================] - 1063s 3ms/step - loss: 0.6768 - acc: 0.5876 - val_loss: 0.6962 - val_acc: 0.5316\n",
      "Epoch 2/6\n",
      "320935/320935 [==============================] - 1048s 3ms/step - loss: 0.6569 - acc: 0.6103 - val_loss: 0.7208 - val_acc: 0.5274\n",
      "Epoch 3/6\n",
      "320935/320935 [==============================] - 1059s 3ms/step - loss: 0.6236 - acc: 0.6426 - val_loss: 0.7422 - val_acc: 0.5234\n",
      "Epoch 4/6\n",
      "320935/320935 [==============================] - 1064s 3ms/step - loss: 0.5843 - acc: 0.6748 - val_loss: 0.8022 - val_acc: 0.5155\n",
      "Epoch 5/6\n",
      "320935/320935 [==============================] - 1062s 3ms/step - loss: 0.5394 - acc: 0.7078 - val_loss: 0.8520 - val_acc: 0.5158\n",
      "Epoch 6/6\n",
      "320935/320935 [==============================] - 1066s 3ms/step - loss: 0.4923 - acc: 0.7393 - val_loss: 0.9511 - val_acc: 0.5140\n",
      "75157/75157 [==============================] - 39s 525us/step\n",
      "('Test score:', 0.95107200784633228)\n",
      "('Test accuracy:', 0.51397740729802976)\n"
     ]
    }
   ],
   "source": [
    "# Select BTC labels for y\n",
    "y_train = y_train_btc\n",
    "y_test = y_test_btc\n",
    "\n",
    "# Run\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, \n",
    "                            y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue running the model (6 epochs was not enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320935 samples, validate on 75157 samples\n",
      "Epoch 1/6\n",
      "320935/320935 [==============================] - 1076s 3ms/step - loss: 0.4444 - acc: 0.7690 - val_loss: 1.0411 - val_acc: 0.5140\n",
      "Epoch 2/6\n",
      "320935/320935 [==============================] - 1068s 3ms/step - loss: 0.3994 - acc: 0.7964 - val_loss: 1.1576 - val_acc: 0.5082\n",
      "Epoch 3/6\n",
      "320935/320935 [==============================] - 1054s 3ms/step - loss: 0.3582 - acc: 0.8216 - val_loss: 1.2720 - val_acc: 0.5097\n",
      "Epoch 4/6\n",
      "320935/320935 [==============================] - 1061s 3ms/step - loss: 0.3220 - acc: 0.8419 - val_loss: 1.4336 - val_acc: 0.5083\n",
      "Epoch 5/6\n",
      "320935/320935 [==============================] - 1073s 3ms/step - loss: 0.2921 - acc: 0.8591 - val_loss: 1.5574 - val_acc: 0.5041\n",
      "Epoch 6/6\n",
      "320935/320935 [==============================] - 1065s 3ms/step - loss: 0.2668 - acc: 0.8734 - val_loss: 1.6760 - val_acc: 0.5055\n",
      "75157/75157 [==============================] - 41s 544us/step\n",
      "('Test score:', 1.6760358921898504)\n",
      "('Test accuracy:', 0.5055417326444327)\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, \n",
    "                            y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b) Model for BTC predictions (using only posts with score > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words per post: 78\n",
      "\n",
      "Getting x_train, y_train, x_test, and y_test...\n",
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (84543, 78))\n",
      "('x_test shape:', (22416, 78))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 78, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 84543 samples, validate on 22416 samples\n",
      "Epoch 1/6\n",
      "84543/84543 [==============================] - 286s 3ms/step - loss: 0.6778 - acc: 0.5861 - val_loss: 0.7026 - val_acc: 0.5221\n",
      "Epoch 2/6\n",
      "84543/84543 [==============================] - 285s 3ms/step - loss: 0.6453 - acc: 0.6244 - val_loss: 0.7360 - val_acc: 0.5131\n",
      "Epoch 3/6\n",
      "84543/84543 [==============================] - 286s 3ms/step - loss: 0.5749 - acc: 0.6887 - val_loss: 0.8153 - val_acc: 0.5152\n",
      "Epoch 4/6\n",
      "84543/84543 [==============================] - 286s 3ms/step - loss: 0.4949 - acc: 0.7410 - val_loss: 0.9299 - val_acc: 0.5101\n",
      "Epoch 5/6\n",
      "84543/84543 [==============================] - 283s 3ms/step - loss: 0.4190 - acc: 0.7834 - val_loss: 1.0652 - val_acc: 0.5092\n",
      "Epoch 6/6\n",
      "84543/84543 [==============================] - 286s 3ms/step - loss: 0.3520 - acc: 0.8192 - val_loss: 1.3103 - val_acc: 0.5121\n",
      "22416/22416 [==============================] - 12s 535us/step\n",
      "('Test score:', 1.7078619607425773)\n",
      "('Test accuracy:', 0.49741256245538901)\n"
     ]
    }
   ],
   "source": [
    "# Load in the csv data\n",
    "reddit_s10_train_df = pd.read_csv(\"../../data/reddit/labeled/score10_all_sub_labeled_train.csv\", index_col=0)\n",
    "reddit_s10_test_df = pd.read_csv(\"../../data/reddit/labeled/score10_all_sub_labeled_dev.csv\", index_col=0)\n",
    "\n",
    "# Determine max title length\n",
    "max_words_train = get_max_words(reddit_s10_train_df.title.values)\n",
    "max_words_test = get_max_words(reddit_s10_test_df.title.values)\n",
    "max_words_s10 = max(max_words_train, max_words_test)\n",
    "print(\"Max number of words per post: {}\".format(max_words_s10))\n",
    "\n",
    "# Split into x_train and y_train\n",
    "print('\\nGetting x_train, y_train, x_test, and y_test...')\n",
    "(x_train_s10, y_train_btc_s10, y_train_eth_s10, y_train_ltc_s10) = reddit_s10_train_df[TEXT_COL].values, \\\n",
    "    reddit_s10_train_df[BTC_LABEL_COL], reddit_s10_train_df[ETH_LABEL_COL], reddit_s10_train_df[LTC_LABEL_COL]\n",
    "(x_test_s10, y_test_btc_s10, y_test_eth_s10, y_test_ltc_s10) = reddit_s10_test_df[TEXT_COL].values, \\\n",
    "    reddit_s10_test_df[BTC_LABEL_COL], reddit_s10_test_df[ETH_LABEL_COL], reddit_s10_test_df[LTC_LABEL_COL]\n",
    "\n",
    "# New model setup hyperparameter\n",
    "maxlen_s10 = max_words_s10  # cut texts after this number of words (among top max_features most common words) \n",
    "\n",
    "# First train our Tokenizer to create a vocabulary of words\n",
    "tokenizer_s10 = Tokenizer(num_words=max_features)\n",
    "tokenizer_s10.fit_on_texts(x_train_s10)\n",
    "\n",
    "# Second vectorize each headline\n",
    "train_sequences_s10 = tokenizer_s10.texts_to_sequences(x_train_s10)\n",
    "test_sequences_s10 = tokenizer_s10.texts_to_sequences(x_test_s10)\n",
    "\n",
    "# For an RNN, ou need a 'sequence' of data as the input\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train_s10 = sequence.pad_sequences(train_sequences_s10, maxlen=maxlen_s10)\n",
    "x_test_s10 = sequence.pad_sequences(test_sequences_s10, maxlen=maxlen_s10)\n",
    "print('x_train shape:', x_train_s10.shape)\n",
    "print('x_test shape:', x_test_s10.shape)\n",
    "\n",
    "# Construct the model\n",
    "input_layer = Input(shape=(maxlen_s10,), dtype='int32', name='main_input')\n",
    "embedding_layer = Embedding(output_dim=128, \n",
    "                            input_dim=max_features, \n",
    "                            input_length=maxlen_s10)(input_layer)\n",
    "lstm_layer = LSTM(128)(embedding_layer)\n",
    "output = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "model_btc_s10 = Model(inputs=input_layer, \n",
    "                       outputs=output)\n",
    "model_btc_s10.summary()\n",
    "model_btc_s10.compile(loss='binary_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Select BTC s10 labels for y\n",
    "y_train_s10 = y_train_btc_s10\n",
    "y_test_s10 = y_test_btc_s10\n",
    "\n",
    "# Run the model\n",
    "model_btc_s10.fit(x_train_s10, \n",
    "          y_train_s10, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_test_s10, y_test_s10))\n",
    "\n",
    "score, acc = model.evaluate(x_test_s10, \n",
    "                            y_test_s10,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue running the model (6 epochs was not enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 84543 samples, validate on 22416 samples\n",
      "Epoch 1/6\n",
      "84543/84543 [==============================] - 306s 4ms/step - loss: 0.2973 - acc: 0.8486 - val_loss: 1.6951 - val_acc: 0.5050\n",
      "Epoch 2/6\n",
      "84543/84543 [==============================] - 324s 4ms/step - loss: 0.2531 - acc: 0.8738 - val_loss: 1.8212 - val_acc: 0.5060\n",
      "Epoch 3/6\n",
      "84543/84543 [==============================] - 326s 4ms/step - loss: 0.2160 - acc: 0.8960 - val_loss: 2.1299 - val_acc: 0.5064\n",
      "Epoch 4/6\n",
      "84543/84543 [==============================] - 307s 4ms/step - loss: 0.1831 - acc: 0.9139 - val_loss: 2.4458 - val_acc: 0.5050\n",
      "Epoch 5/6\n",
      "84543/84543 [==============================] - 282s 3ms/step - loss: 0.1558 - acc: 0.9281 - val_loss: 2.5559 - val_acc: 0.5049\n",
      "Epoch 6/6\n",
      "84543/84543 [==============================] - 284s 3ms/step - loss: 0.1341 - acc: 0.9394 - val_loss: 2.6928 - val_acc: 0.5025\n",
      "22416/22416 [==============================] - 11s 506us/step\n",
      "('Test score:', 1.7078619607425773)\n",
      "('Test accuracy:', 0.49741256245538901)\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "model_btc_s10.fit(x_train_s10, \n",
    "          y_train_s10, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_test_s10, y_test_s10))\n",
    "\n",
    "score, acc = model.evaluate(x_test_s10, \n",
    "                            y_test_s10,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6) Model for ETH predictions (using Keras functional API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API version of the same model\n",
    "# The functional API is much more useful than the sequential API in terms of adaptability \n",
    "input_layer = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "\n",
    "embedding_layer = Embedding(output_dim=128, input_dim=max_features, input_length=maxlen)(input_layer)\n",
    "\n",
    "lstm_layer = LSTM(128)(embedding_layer)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "model_eth = Model(inputs=input_layer, \n",
    "              outputs=output)\n",
    "model_eth.summary()\n",
    "\n",
    "model_eth.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ETH labels for y\n",
    "y_train = y_train_eth\n",
    "y_test = y_test_eth\n",
    "\n",
    "# Run\n",
    "model_eth.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model_eth.evaluate(x_test, \n",
    "                            y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Model for LTC predictions (using Keras functional API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API version of the same model\n",
    "# The functional API is much more useful than the sequential API in terms of adaptability \n",
    "input_layer = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "\n",
    "embedding_layer = Embedding(output_dim=128, input_dim=max_features, input_length=maxlen)(input_layer)\n",
    "\n",
    "lstm_layer = LSTM(128)(embedding_layer)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "model_ltc = Model(inputs=input_layer, \n",
    "              outputs=output)\n",
    "model_ltc.summary()\n",
    "\n",
    "model_ltc.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select LTC labels for y\n",
    "y_train = y_train_ltc\n",
    "y_test = y_test_ltc\n",
    "\n",
    "# Run\n",
    "model_ltc.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model_ltc.evaluate(x_test, \n",
    "                            y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
